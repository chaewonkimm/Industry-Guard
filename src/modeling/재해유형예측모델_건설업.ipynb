{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "600e1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5f6f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('건설업전처리완료(target수정).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dddd5331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'SQ1', 'SQ2', 'Q1_1A', 'Q1_1B', 'Q1_1C', 'Q1_2A', 'Q1_2B',\n",
       "       'Q1_2C', 'Q1_3', 'Q1_4', 'Q2', 'Q3_1_1', 'Q4_1', 'Q4_2', 'Q4_3', 'Q4_4',\n",
       "       'Q5', 'Q6', 'Q9', 'Q11', 'Q13_1', 'Q13_2', 'Q13_3', 'Q13_4', 'Q13_5',\n",
       "       'Q13_6', 'Q13_7', 'Q13_8', 'Q13_9', 'Q13_10', 'Q13_11', 'Q13_12',\n",
       "       'Q13_13', 'Q13_14', 'Q13_15', 'Q13_16', 'Q13_17', 'Q13_18', 'Q13_19',\n",
       "       'Q13_20', 'Q16_1', 'Q16_4', 'Q16_13', 'Q16_14', 'Q16_15', 'Q22',\n",
       "       'Q28_1', 'target', 'duration', '55_ratio', '55_plus', '55_mul',\n",
       "       'foreign_ratio', 'women_ratio', 'syear_type', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0bd6cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "276da5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "X = df.drop(columns='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e624ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "scores = []\n",
    "oof_pred = np.zeros(X_test.shape[0])  # OOF 저장\n",
    "kfold = KFold(n_splits=13, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, valid_index in kfold.split(X_train, y_train):\n",
    "    train_x, valid_x = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    train_y, valid_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    # LightGBM 모델링\n",
    "    model = LGBMClassifier(random_state=0)\n",
    "    model.fit(train_x, train_y,\n",
    "              eval_set=[(valid_x, valid_y)],\n",
    "              early_stopping_rounds=100,\n",
    "              verbose=True\n",
    "              )\n",
    "\n",
    "    y_pred = model.predict_proba(valid_x)[:, 1]\n",
    "    auc = roc_auc_score(valid_y, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "    oof_pred += model.predict_proba(X_test)[:, 1] / kfold.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "48de9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:  [0.78322581 0.80185185 0.74816176 0.82412399 0.83264746 0.77055703\n",
      " 0.81432361 0.79225806 0.76223776 0.79281046 0.72903226 0.76888021\n",
      " 0.80730223]\n",
      "CV mean = 0.79 with std = 0.03\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores) \n",
    "print(\"CV scores: \", scores)\n",
    "print(\"CV mean = %.2f\" % scores.mean(), \"with std = %.2f\" % scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7eacd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "scores = []\n",
    "oof_pred = np.zeros(X_test.shape[0])  # OOF 저장\n",
    "kfold = KFold(n_splits=13, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, valid_index in kfold.split(X_train, y_train):\n",
    "    train_x, valid_x = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    train_y, valid_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    # RandomForest 모델링\n",
    "    model = RandomForestClassifier(random_state=0)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    y_pred = model.predict_proba(valid_x)[:, 1]\n",
    "    auc = roc_auc_score(valid_y, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "    oof_pred += model.predict_proba(X_test)[:, 1] / kfold.get_n_splits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4629e72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:  [0.77129032 0.81419753 0.69852941 0.8143531  0.83367627 0.75928382\n",
      " 0.82228117 0.76870968 0.7527972  0.80915033 0.76290323 0.79003906\n",
      " 0.81000676]\n",
      "CV mean = 0.79 with std = 0.04\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores) \n",
    "print(\"CV scores: \", scores)\n",
    "print(\"CV mean = %.2f\" % scores.mean(), \"with std = %.2f\" % scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af1ac193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "scores = []\n",
    "oof_pred = np.zeros(X_test.shape[0])  # OOF 저장\n",
    "kfold = KFold(n_splits=13, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, valid_index in kfold.split(X_train, y_train):\n",
    "    train_x, valid_x = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    train_y, valid_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    # XGBoost 모델링\n",
    "    model = XGBClassifier(random_state=0)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    y_pred = model.predict_proba(valid_x)[:, 1]\n",
    "    auc = roc_auc_score(valid_y, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "    oof_pred += model.predict_proba(X_test)[:, 1] / kfold.get_n_splits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a5eaad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:  [0.73806452 0.73888889 0.75091912 0.78975741 0.81961591 0.74204244\n",
      " 0.77785146 0.73032258 0.73776224 0.76339869 0.69419355 0.76041667\n",
      " 0.74171738]\n",
      "CV mean = 0.75 with std = 0.03\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores) \n",
    "print(\"CV scores: \", scores)\n",
    "print(\"CV mean = %.2f\" % scores.mean(), \"with std = %.2f\" % scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a88a6a",
   "metadata": {},
   "source": [
    "### Modeling(튜닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f16cccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class CatBoostLGBMRFXGB:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.cat_study = None\n",
    "        self.lgb_study = None\n",
    "        self.rf_study = None\n",
    "        self.xgb_study = None\n",
    "        self.cat_best_params = None\n",
    "        self.lgb_best_params = None\n",
    "        self.rf_best_params = None\n",
    "        self.xgb_best_params = None\n",
    "\n",
    "    def cat_objective(self, trial):\n",
    "        params_cat = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
    "            'bagging_temperature': trial.suggest_int('bagging_temperature', 0, 100),\n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait': trial.suggest_int('od_wait', 10, 50)\n",
    "        }\n",
    "\n",
    "        model = CatBoostClassifier(**params_cat, random_state=42)\n",
    "        scores = cross_val_score(model, self.X_train, self.y_train, scoring='roc_auc', n_jobs=-1, cv=5)\n",
    "        score = sum(scores) / len(scores)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def lgb_objective(self, trial):\n",
    "        params_lgb = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 6, 20),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1500, 3000, step=1),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 1.0),\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(random_state=42, **params_lgb)\n",
    "        scores = cross_val_score(model, self.X_train, self.y_train, scoring='roc_auc', n_jobs=-1, cv=5)\n",
    "        score = sum(scores) / len(scores)\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def rf_objective(self, trial):\n",
    "        params_rf = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 500),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'n_estimators':  trial.suggest_int('n_estimators', 100, 1000)\n",
    "        }\n",
    "\n",
    "        model = RandomForestClassifier(random_state=42, **params_rf)\n",
    "        scores = cross_val_score(model, self.X_train, self.y_train, scoring='roc_auc', n_jobs=-1, cv=5)\n",
    "        score = sum(scores) / len(scores)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def xgb_objective(self, trial):\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 6, 16),\n",
    "            \"learning_rate\": trial.suggest_float('learning_rate', 0.05, 0.99),\n",
    "            'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000, step=100),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 1, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 1, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.05),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = XGBClassifier(random_state=42, **params)\n",
    "        scores = cross_val_score(model, self.X_train, self.y_train, scoring='roc_auc', n_jobs=-1, cv=5)\n",
    "        score = sum(scores) / len(scores)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def optimize_catboost(self, n_trials=200):\n",
    "        self.cat_study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        self.cat_study.optimize(self.cat_objective, n_trials=n_trials)\n",
    "        self.cat_best_params = self.cat_study.best_params\n",
    "\n",
    "    def optimize_lgbm(self, n_trials=200):\n",
    "        self.lgb_study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        self.lgb_study.optimize(self.lgb_objective, n_trials=n_trials)\n",
    "        self.lgb_best_params = self.lgb_study.best_params\n",
    "    \n",
    "    def optimize_rf(self, n_trials=200):\n",
    "        self.rf_study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        self.rf_study.optimize(self.rf_objective, n_trials=n_trials)\n",
    "        self.rf_best_params = self.rf_study.best_params\n",
    "    \n",
    "    def optimize_xgb(self, n_trials=200):\n",
    "        self.xgb_study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42), direction='maximize')\n",
    "        self.xgb_study.optimize(self.xgb_objective, n_trials=n_trials)\n",
    "        self.xgb_best_params = self.xgb_study.best_params\n",
    "\n",
    "    def train_catboost(self):\n",
    "        CAT = CatBoostClassifier(**self.cat_best_params, random_state=42, verbose=False)\n",
    "        CAT.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def train_lgbm(self):\n",
    "        LGBM = LGBMClassifier(**self.lgb_best_params, random_state=42)\n",
    "        LGBM.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def train_rf(self):\n",
    "        RF = RandomForestClassifier(**self.rf_best_params, random_state=42)\n",
    "        RF.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def train_xgb(self):\n",
    "        XGB = XGBClassifier(**self.xgb_best_params, random_state=42)\n",
    "        XGB.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def predict_catboost(self):\n",
    "        CAT = CatBoostClassifier(**self.cat_best_params, random_state=42, verbose=False)\n",
    "        CAT.fit(self.X_train, self.y_train)\n",
    "        cat_pred = CAT.predict_proba(self.X_test)[:, 1]\n",
    "        score = roc_auc_score(self.y_test, cat_pred)\n",
    "        return score\n",
    "\n",
    "    def predict_lgbm(self):\n",
    "        LGBM = LGBMClassifier(**self.lgb_best_params, random_state=42)\n",
    "        LGBM.fit(self.X_train, self.y_train)\n",
    "        lgbm_pred = LGBM.predict_proba(self.X_test)[:, 1]\n",
    "        score = roc_auc_score(self.y_test, lgbm_pred)\n",
    "        return score\n",
    "    \n",
    "    def predict_rf(self):\n",
    "        RF = RandomForestClassifier(**self.rf_best_params, random_state=42)\n",
    "        RF.fit(self.X_train, self.y_train)\n",
    "        rf_pred = RF.predict_proba(self.X_test)[:, 1]\n",
    "        score = roc_auc_score(self.y_test, rf_pred)\n",
    "        return score\n",
    "    \n",
    "    def predict_xgb(self):\n",
    "        XGB = XGBClassifier(**self.xgb_best_params, random_state=42)\n",
    "        XGB.fit(self.X_train, self.y_train)\n",
    "        xgb_pred = XGB.predict_proba(self.X_test)[:, 1]\n",
    "        score = roc_auc_score(self.y_test, xgb_pred)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostLGBMRFXGB(X_train, y_train, X_test, y_test)\n",
    "clf.optimize_catboost(n_trials=200)\n",
    "clf.optimize_lgbm(n_trials=200)\n",
    "clf.optimize_rf(n_trials=200)\n",
    "clf.optimize_xgb(n_trials=200)\n",
    "clf.train_catboost()\n",
    "clf.train_lgbm()\n",
    "clf.train_rf()\n",
    "clf.train_xgb()\n",
    "catboost_score = clf.predict_catboost()\n",
    "lgbm_score = clf.predict_lgbm()\n",
    "rf_score = clf.predict_rf()\n",
    "xgb_score = clf.predict_xgb()\n",
    "print(\"CatBoost Score:\", catboost_score)\n",
    "print(\"LightGBM Score:\", lgbm_score)\n",
    "print(\"Random Forest Score:\", rf_score)\n",
    "print(\"XGBoost Score:\", xgb_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
